# Fine-Tune-Flan-T5-Model-for-Dialogue-Summarization
In this notebook, an existing LLM from Hugging Face will be fine-tuned for enhanced dialogue summarization. The FLAN-T5 model, which is a high-quality instruction-tuned model capable of summarizing text out of the box, will be used. To improve inferences, a full fine-tuning approach will be explored, and the results will be evaluated using ROUGE metrics. Following this, Parameter Efficient Fine-Tuning (PEFT) will be performed, the resulting model will be evaluated, and it will be demonstrated that the benefits of PEFT outweigh the slightly lower performance metrics.
-- Done as a part of the "Genarative AI with LLMs" course by AWS and Coursera --
